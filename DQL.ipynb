{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport mxnet as mx\nimport numpy as np\nimport cv2\nmodel = torch.hub.load('pytorch/vision:v0.4.2', 'resnet50', pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport torchvision.transforms.functional as TF\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\ndef Action(image_Tensor, cord, action, increment):\n    cord_new = cord.clone()\n\n    # image_Tensor = Image tensor[3,224,224]\n\n    # cord = [xl,xh,yl,yh]\n\n    # action = 0: increase xl by increment\n    # action = 1: decrease xl by increment\n    # action = 2: increase xh by increment\n    # action = 3: decrease xh by increment\n    # action = 4: increase yl by increment\n    # action = 5: decrease yl by increment\n    # action = 6: increase yl by increment\n    # action = 7: decrease yl by increment\n    # action = 8: Trigger\n\n    # increment = movmenet value\n\n    pil2tensor = transforms.ToTensor()\n    tensor2pil = transforms.ToPILImage()\n\n    transform = transforms.Compose(\n        [transforms.Resize([224, 224])])\n\n    if action == 0:\n        cord_new[0] += increment\n    if action == 1:\n        cord_new[0] -= increment\n    if action == 2:\n        cord_new[1] += increment\n    if action == 3:\n        cord_new[1] -= increment\n    if action == 4:\n        cord_new[2] += increment\n    if action == 5:\n        cord_new[2] -= increment\n    if action == 6:\n        cord_new[3] += increment\n    if action == 7:\n        cord_new[3] -= increment\n\n    cord_new[0] = np.minimum(222, cord_new[0])\n    cord_new[1] = np.minimum(223, cord_new[1])\n    if cord_new[0] >= cord_new[1]:\n        cord_new[0] = cord_new[1] - 1\n    cord_new[2] = np.minimum(222, cord_new[2])\n    cord_new[3] = np.minimum(223, cord_new[3])\n    if cord_new[2] >= cord_new[3]:\n        cord_new[2] = cord_new[3] - 1\n\n    Img_cut = image_Tensor[:, cord_new[0]:cord_new[1], cord_new[2]:cord_new[3]]\n    Img_cut=Img_cut.cpu()\n    plt.figure()\n    plt.imshow(Img_cut.numpy().transpose(1, 2, 0))\n    plt.show()\n    print(image_Tensor.type())\n    Img_PIL =transforms.functional.to_pil_image(image_Tensor.cpu(), mode=\"RGB\")\n    plt.figure()\n    print(Img_PIL)\n    plt.imshow(Img_PIL)\n    plt.show()\n    Img_PIL = transform(Img_PIL)\n\n    Img_new = pil2tensor(Img_PIL)\n    print(Img_new)\n\n    return Img_new, cord_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nimport torch.nn.functional as F\n\n# Based on Resnet implementation from: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\n# These layers are based on DetNet: A Backbone network for Object Detection, https://arxiv.org/pdf/1804.06215.pdf\nclass DetnetBottleneck(nn.Module):\n    # We keep the same grid size in the output. (SxS)\n    # Layer structre is 1x1 conv, dilated 3x3 conv, 1x1 conv, with a skip connection\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, block_type='A'):\n        super(DetnetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=2, bias=False, dilation=2)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.downsample = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes or block_type == 'B':\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.downsample(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.layer6=nn.Linear(100352,2048)\n        self.layer7=nn.Linear(2048,1024)\n        self.layer8=nn.Linear(1024,20)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = [block(self.inplanes, planes, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n    \n    def _make_detnet_layer(self,in_channels):\n        layers = []\n        layers.append(DetnetBottleneck(in_planes=in_channels, planes=256, block_type='B'))\n        layers.append(DetnetBottleneck(in_planes=256, planes=256, block_type='A'))\n        layers.append(DetnetBottleneck(in_planes=256, planes=256, block_type='A'))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)        \n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.view(batch_size, -1) \n        x = self.layer6(x)\n        x = self.relu(x)\n        features = self.layer7(x)\n        x=self.relu(features)\n        x=self.layer8(x)\n        return torch.nn.functional.softmax(x[0],dim=0),features\n\n\ndef update_state_dict(pretrained_model, model):\n    new_state_dict = pretrained_model.state_dict()\n    dd = model.state_dict()\n    for k in new_state_dict.keys():\n        if k in dd.keys() and not k.startswith('fc'):\n            dd[k] = new_state_dict[k]\n    model.load_state_dict(dd)\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        pretrained_model = models.resnet50(pretrained=True)\n        model = update_state_dict(pretrained_model, model)\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport torchvision.transforms.functional as TF\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport math\n\n\ndef Action(image_Tensor, cord, action, increment):\n    cord_new = cord.clone().detach()\n    # image_Tensor = Image tensor[3,224,224]\n    special_increment=int(math.sqrt((increment*increment)/2))\n    # cord = [x1,y1,x2,y2]\n\n\n\n    # increment = movmenet value\n\n    pil2tensor = transforms.ToTensor()\n    tensor2pil = transforms.ToPILImage()\n    action_type=\"\"\n    transform = transforms.Compose(\n        [transforms.Resize([224, 224])])\n\n    if action == 0: # move the box right\n        action_type=\"move right\"\n        cord_new[0] += increment\n        cord_new[2] += increment\n    if action == 1: # move the box left\n        action_type=\"move left\"        \n        cord_new[0] -= increment\n        cord_new[2] -= increment\n    if action == 2: # move the box up\n        action_type=\"move up\"\n        cord_new[1] += increment\n        cord_new[3] += increment\n    if action == 3: # move the box down\n        action_type=\"move down\"\n        cord_new[1] -= increment\n        cord_new[3] -= increment\n    if action == 4: # zoom in\n        action_type=\"zoom in\"\n        cord_new[0] += special_increment\n        cord_new[1] += special_increment\n        cord_new[2] -= special_increment\n        cord_new[3] -= special_increment\n    if action == 5: # zoom out\n        action_type=\"zoom out\"\n        cord_new[0] -= special_increment\n        cord_new[1] -= special_increment\n        cord_new[2] += special_increment\n        cord_new[3] += special_increment\n        \n    cord_new[0] = np.maximum(0, cord_new[0])\n    cord_new[1] = np.maximum(0, cord_new[1])\n    cord_new[2] = np.maximum(0, cord_new[2])\n    cord_new[3] = np.maximum(0, cord_new[3])\n        \n    cord_new[0] = np.minimum(223, cord_new[0])\n    cord_new[2] = np.minimum(223, cord_new[1])\n    if cord_new[0] >= cord_new[2]:\n        cord_new[0] = cord_new[2] - 1\n    cord_new[1] = np.minimum(223, cord_new[2])\n    cord_new[3] = np.minimum(223, cord_new[3])\n    if cord_new[1] >= cord_new[3]:\n        cord_new[1] = cord_new[3] - 1\n    \n    if(cord_new[2]-cord_new[0]<=50):\n        if(cord_new[2]<223-(50-cord_new[2]+cord_new[0])):\n            cord_new[2] += (50-cord_new[2]+cord_new[0])\n        else:\n            cord_new[2] -= (50-cord_new[2]+cord_new[0])\n            \n    if(cord_new[3]-cord_new[1]<=50):\n        if(cord_new[3]<223-(50-cord_new[3]+cord_new[1])):\n            cord_new[3] += (50-cord_new[3]+cord_new[1])\n        else:\n            cord_new[3] -= (50-cord_new[3]+cord_new[1])\n\n            \n\n    \n    if(cord_new[2]-cord_new[0]<=50):\n        if(cord_new[2]+40<223):\n            cord_new[2] += 40\n        else:\n            cord_new[0] -= 40\n            \n    if(cord_new[3]-cord_new[1]<=50):\n        if(cord_new[3]<+40<223):\n            cord_new[3] += 40\n        else:\n            cord_new[1] -= 40\n    \n    cord_new[0] = np.maximum(0, cord_new[0])\n    cord_new[1] = np.maximum(0, cord_new[1])\n    cord_new[2] = np.maximum(0, cord_new[2])\n    cord_new[3] = np.maximum(0, cord_new[3])\n    Img_cut = image_Tensor[:, cord_new[1]:cord_new[3], cord_new[0]:cord_new[2]]\n#     plt.figure()\n#     plt.imshow(Img_cut.cpu().numpy().transpose(1, 2, 0))\n#     plt.show()\n#    print([cord_new[0],cord_new[1], cord_new[2],cord_new[3]])\n    Img_cut=Img_cut.unsqueeze(0)\n    Img_new=F.interpolate(Img_cut, size=(224,224), mode='bilinear')\n\n    return Img_new, cord_new, action_type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# net = resnet50(pretrained=True).cuda()\n# from PIL import Image\n# import os\n# from torchvision import transforms\n# results=[]\n# preprocess = transforms.Compose([\n#     transforms.Resize(256),\n#     transforms.CenterCrop(224),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n# ])\n# for file in os.listdir(\"../input/testyolonetwork/test/JPEGImages/\"):\n#     input_image = Image.open(\"../input/testyolonetwork/test/JPEGImages/\"+file)\n#     print(img)\n\n#     input_tensor = preprocess(input_image)\n#     input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n\n#     # move the input and model to GPU for speed if available\n#     if torch.cuda.is_available():\n#         input_batch = input_batch.to('cuda')\n#         model.to('cuda')\n\n#     with torch.no_grad():\n#         print(input_batch.shape)\n#         confidence,features=net(input_batch)\n#         print(confidence.shape)\n#         print(confidence)\n#         print(features.shape)\n#         output = model(input_batch)\n#     # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n#     #print(output[0])\n#     # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n#     probs=torch.nn.functional.softmax(output[0], dim=0)\n#     values, indices = torch.max(probs, 0)\n#     print(\"here\")\n#     print(values)\n#     results.append(indices)\n# resultsfinal=np.array(results)\n# print(np.unique(resultsfinal))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_iou (box1, box2):                                                                                                                                                             \n        '''Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n        Args:\n          box1: (tensor) bounding boxes, sized [N,4].\n          box2: (tensor) bounding boxes, sized [M,4].\n        Return:\n          (tensor) iou, sized [N,M].\n        '''\n\n        N = box1.size(0)\n        M = box2.size(0)\n \n        lt = torch.max(\n            box1[:,:2].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n            box2[:,:2].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n        )   \n \n        rb = torch.min(\n            box1[:,2:].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n            box2[:,2:].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n        )   \n \n        wh = rb - lt  # [N,M,2]\n        wh[wh<0] = 0  # clip at 0\n        inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\n \n        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\n        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\n        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n \n        iou = inter / (area1 + area2 - inter)\n        return iou","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport random\n\n# parameter\ngamma = 0.1\nepsilon_min = 0.1 # minimum exploration value (the smaller the more exploitation)\nepsilon_max = 1.0 # maximum exploration value (the larger the more exploration)\nepsilon = epsilon_max\nepsilon_dec_steps = 5\nepsilon_dec = (epsilon_max - epsilon_min) / epsilon_dec_steps\ntarget_update_interval = 50\nmax_step = 500\nexperience = [] # experience replay\nexperience_buffer_size = 2000\nexperience_sample_size = 15\n\n# class for defining a state\nclass State:\n    def __init__(self, input_tensor):\n        self.input_tensor = input_tensor\n\n    def state_feature(self):\n        return self.input_tensor\n\n\n# Q_learning model\ndef DQL_model(input=1024, output=6):\n    \"\"\"\n    Initialize and return a simple discriminator model.\n    \"\"\"\n    model = torch.nn.Sequential(torch.nn.Linear(in_features=input, out_features=1024),\n                                torch.nn.ReLU(),\n                                torch.nn.Linear(in_features=1024, out_features=output)\n            )\n    return model\n\n\n# calculate q_values of given state input\ndef get_q_values(state, model):\n    input_features = state.cuda()\n    q_values = model(input_features)\n    #q_values = predict_q_values.detach().numpy()\n    return q_values\n\n\n# select action base on q_values of current state\n# there are 9 possible action indexed from 0 -> 8\ndef select_action(epsilon, action_values):\n    # do exploitation if epsilon less than random in range (0,1)\n    if random.random() > epsilon:\n        action = action_values.max(0)[1]\n\n    # do exploration if epsilon greater than random in range (0,1)\n    # generate random action index between 0 and 8\n    else:\n        action = random.randint(0,9)\n\n    return action\n\n\n# place holder for take_action function need Sina and Arhm function for take_action\ndef take_action(image_tensor,bbox, action):\n    #using the initial_bbox to transform the image using a function\n    ############\n    # reward is the confidence score\n    next_image,next_bbox,action_type= Action(image_tensor,bbox,action,20) #Ahm function return\n    \n    return next_image,next_bbox,action_type\n##########################################################\n\n\n# compute next state in target network\ndef compute_target(r, next_state, target_model):\n    return r + gamma * (get_q_values(next_state.state_feature(), target_model)).max(0)[1]\n\n\n# using experience inside experience_replay to train the model\ndef apply_experience(experience, Qmodel, target_model,flag):\n    random.shuffle(experience)\n    if len(experience) < experience_sample_size:\n         sample_size = len(experience)\n    else:\n         sample_size = experience_sample_size\n    i=0\n    for random_exp in experience[0:sample_size]:\n        current_state, action_values, action, state_reward, next_state, stopping_condition = random_exp\n        # compute q_values of next state at target model\n        target_value = compute_target(state_reward, next_state, target_model)\n        if (stopping_condition != True):\n            # Compute and print loss\n            loss = criterion(torch.max(action_values[0]), torch.max(target_value))\n            # Zero gradients, perform a backward pass, and update the weights.\n            optimizer.zero_grad()\n            if(flag==False and i<=sample_size):\n                loss.backward(retain_graph=True)\n                optimizer.step()\n                break\n            else:\n                loss.backward(retain_graph=True)\n                optimizer.step()\n            i=i+1\n\n\n#update the target_model with weight from Qmodel\ndef update_target_model(Qmodel, target_model):\n    target_model.load_state_dict(Qmodel.state_dict())\n\n\n# initiate model, loss function and optimizer\nQmodel =  DQL_model().cuda()\ntarget_model = DQL_model().cuda()\ncriterion = nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(Qmodel.parameters(), lr=1e-4)\n\n\n# training function for deep Q network, this return the current epsilon to keep track\ndef DQL_train (ground_truth,init_input_image,state_features, Qmodel, target_model, box_coor,current_epoch, epsilon=epsilon_max):\n\n    # coordinate of the box in list format [x1,y1,x2,y2]\n    initial_bbox = box_coor\n    bbox = initial_bbox\n    # using the initial_bbox to transform the image using a function\n    # Sina function\n    ############\n    step = 0\n    total_step = 0\n    # create initial state base on parsing state_features\n    current_state = State(state_features)\n    stopping_condition = False\n    ious=[]\n    bboxes=[]\n    # keep exploring the original image until either max_step is reached or the object has been identified\n    while (step < max_step or stopping_condition != True):\n        # compute q values and decide the action base on q values of the current state\n        action_values = get_q_values(current_state.state_feature().cuda(), Qmodel)\n        action = select_action(epsilon, action_values)\n        # apply action to the current state to get the next state and reward and check for stopping condition and epoch#\n        next_image, next_bbox,action_type= take_action(init_input_image,bbox, action)\n        box1=next_bbox.unsqueeze(0)\n        box2=ground_truth.unsqueeze(0)\n        boundary_box_pred=box1\n        ready_box_pred=torch.zeros([1, 4], dtype=torch.float)\n        ready_box_pred[:,:2]=boundary_box_pred[:,:2]/14 - 0.5*boundary_box_pred[:,2:4]\n        ready_box_pred[:,2:4]=boundary_box_pred[:,:2]/14 + 0.5*boundary_box_pred[:,2:4]\n\n        boundary_box_target=box2\n        ready_box_target=torch.zeros([1,4], dtype=torch.float)\n        ready_box_target[:,:2]=boundary_box_target[:,:2]/14-0.5*boundary_box_target[:,2:4]\n        ready_box_target[:,2:4]=boundary_box_target[:,:2]/14+0.5*boundary_box_target[:,2:4]\n        ious.append(compute_iou(ready_box_pred,ready_box_target))\n        with torch.no_grad():\n            confidence,next_state= net(next_image)\n        state_reward=0\n        if (confidence[5]>0.7):\n            state_reward=1\n            stopping_condition=True\n        else:\n            state_reward=-0.5\n        next_state=State(next_state)\n        step_experience = (current_state, action_values, action, state_reward, next_state, stopping_condition)\n\n        # save the state action sequence into experience replay\n        experience.append(step_experience)\n\n        # apply experience replay\n        if(step+1<max_step and stopping_condition!=True):\n            apply_experience(experience,Qmodel,target_model,True)\n        else:\n            print(\"breaking at\")\n            print(step)\n            apply_experience(experience,Qmodel,target_model,False)\n\n\n        # move to the next state\n        current_state = next_state\n        bbox = next_bbox\n        bboxes.append(bbox)\n        step += 1\n        if step>max_step or stopping_condition==True:\n            break\n        total_step += 1\n        #print(action_type)\n        if(total_step%100==0):\n            print(total_step)\n\n        # if reaching the step update number (10) then update target model with weight from target_model\n        if total_step % target_update_interval  == 0:\n            update_target_model(Qmodel, target_model)\n\n    # reduce exploration parameter as the we becoming more confident in our action\n    if current_epoch < epsilon_dec_steps:\n        epsilon -= epsilon_dec\n\n    return epsilon,bboxes,torch.FloatTensor(ious),\n\n# testing function for deep Q network after training\ndef DQL_testing(state_features, box_coor, Qmodel):\n\n    # coordinate of the box in list format [x1,y1,x2,y2]\n    initial_bbox = box_coor\n    bbox = initial_bbox\n    # using the initial_bbox to transform the image using a function\n    # Sina function\n    ############\n    step = 0\n    total_step = 0\n    # create initial state base on parsing state_features\n    current_state = State(state_features)\n    stopping_condition = False\n\n    while step != max_step or stopping_condition != True:\n        # compute q values and decide the action base on q values of the current state\n        action_values = get_q_values(current_state.state_feature(), Qmodel)\n        action = select_action(epsilon, action_values)\n        # apply action to the current state to get the next state and reward and check for stopping condition and epoch#\n        state_reward, next_state, next_bbox, stopping_condition, current_epoch = take_action(bbox, action)\n\n        # move to the next state\n        current_state = next_state\n        bbox = next_bbox\n        step += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = resnet50(pretrained=True).cuda()\nfeature_optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\nfeature_criterion=torch.nn.BCELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport os\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nresults=[]\npreprocess = transforms.Compose([\n    transforms.Resize([224, 224]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nfile = os.listdir(\"../input/testyolonetwork/test/JPEGImages/\")[15]\ninput_image = Image.open(\"../input/testyolonetwork/test/JPEGImages/\"+file)\nprint(input_image)\ntarget_scores=torch.zeros([20])\ntarget_scores[5]=1\ntargets= open(\"../input/testyolonetwork/test/voc2007.txt\",\"r\")\nlines =targets.read()\ntarget=lines.split(\"\\n\")\nground_truth_str=[]\nfor line in range(len(target)):\n    if file in target[line]:\n        ground_truth_str=target[line].split(\" \")[1:5]\nground_truth=torch.zeros([4], dtype=torch.long)\nground_truth[0]=(int(ground_truth_str[0])/500*224)\nground_truth[1]=(int(ground_truth_str[1])/354*224)\nground_truth[2]=(int(ground_truth_str[2])/500*224)\nground_truth[3]=(int(ground_truth_str[3])/354*224)\nprint(ground_truth)\ninput_tensor = preprocess(input_image)\nplt.figure()\nplt.imshow(input_tensor.numpy().transpose(1, 2, 0))\nplt.show()\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.cuda()\nepisodes=[]\nfor i in range(20):\n    confidences,state_features=net(input_batch)\n    DQN_state_features=state_features.clone().detach()\n    DQN_input_batch=input_batch[0].clone().detach()\n    epsilon,bboxes,ious=DQL_train(ground_truth,DQN_input_batch,DQN_state_features,Qmodel,target_model,torch.tensor([50,100,0,50]),i)\n    value,index=torch.max(ious,0)\n    mask = ious[:] > 0.9\n    #print(\"here\")\n    #print(mask)\n    indices=torch.nonzero(mask)\n    losses=[]\n    torch.cuda.empty_cache()\n    print(\"matches\")\n    print(len(indices))\n    for i in range(len(indices)):\n        next_image,next_bbox,action_type= Action(DQN_input_batch,bboxes[indices[i]],1,0)\n        new_confidences,features=net(next_image)\n        loss=feature_criterion(new_confidences.cpu(),target_scores)\n        print(\"Feature Loss: \")\n        print(loss.item())\n        feature_optimizer.zero_grad()\n        loss.backward()\n        feature_optimizer.step()\n#     if(value>0.7):\n#         episodes.append(bboxes)\n#         box_responsible=bboxes[index]\n#         next_image,next_bbox,action_type= Action(DQN_input_batch,box_responsible,1,0)\n#         new_confidences,features=net(next_image)\n#         loss=feature_criterion(new_confidences.cpu(),target_scores)\n#         print(\"Feature Loss: \")\n#         print(loss.item())\n#         feature_optimizer.zero_grad()\n#         loss.backward()\n#         feature_optimizer.step()\n\n\n    \n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image, ImageDraw\nimg_list=[]\nfor bb in range(len(bboxes)):\n    im=input_image.resize((224,224))\n    rimg_draw = ImageDraw.Draw(im)\n    current_box=bboxes[bb]\n    rimg_draw.rectangle((current_box[0], current_box[1], current_box[2], current_box[3]), fill=None, outline=(255, 0, 0))\n    rimg_draw.rectangle((ground_truth[0], ground_truth[1], ground_truth[2], ground_truth[3]), fill=None, outline=(0, 225, 0))\n    pil2tensor = transforms.ToTensor()\n    img_list.append(pil2tensor(im))\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=100, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}